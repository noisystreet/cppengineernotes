深度学习算法
==================

+ https://harvard-iacs.github.io/2021-CS109B/category/lectures.html
+ https://github.com/sithu31296/sota-backbones
+ https://www.cnblogs.com/moonlight-lin/p/12542317.html
+ https://zhuanlan.zhihu.com/p/464515049

概述
------------------------------------------------

神经网络可以看成是一个关于权重W和输入x的非线性函数y=f(W,x)，又称模型。
为了使该函数具有较高的准确性，需要设置合理的权重，通常给定一组样本数据（即训练集），通过样本数据来拟合权重。这个过程即为训练。
为了评价权重在训练集上的预测效果，定义损失函数loss=L(y,y_true)，代表预测值和样本真实值的差异。
然后整个训练过程就可以看成一个最优化问题，即对于给定的loss函数，求解weight，使得loss在训练集上最小化：
loss=L(y,y_true)
这个过程通常采用基于梯度下降的方法来迭代求解，所以需要求解权重的梯度，计算梯度通常采用自动微分。
得到权重的梯度之后，采用适当的策略更新权重，更新策略即优化器(optimizer)
迭代loss下降到某个阈值时认为满足要求，即收敛。
然后给定一组样本数据（即测试集），使用训练好的模型来计算在测试集上预测的精度。
最后就可以用训练好的模型来进行预测，即推理。

深度学习的典型应用：

+ 图像、视频、文本、语音。
+ SOTA：即state of art，现阶段最优秀的模型，从paper with code网站（https://paperswithcode.com/sota）可以了解到目前一些数据集上表现最好的深度学习模型。

一些名词：

+ 过拟合：模型在训练集上表现很好，在测试集上表现差。
+ 正则化：改善过拟合的手段的统称。
+ 网络退化：网络增加到一定深度时，loss不降反升。
+ 梯度爆炸/消失：在传播过程中梯度值不断增大/缩减为零。

模型与收敛
------------------------------------------------

+ https://zhuanlan.zhihu.com/p/459501430
+ https://zhuanlan.zhihu.com/p/48016051
+ https://www.zhihu.com/question/337513515/answer/768632471
+ https://www.zhihu.com/question/41037974/answer/2549676265
+ https://zhuanlan.zhihu.com/p/595716023

模型常用结构
------------------------------------------------

卷积和池化
````````````````````````````````````````````````

+ https://zhuanlan.zhihu.com/p/87763131
+ https://www.zhihu.com/question/267304069/answer/2569822986
+ https://zhuanlan.zhihu.com/p/113285797
+ https://zhuanlan.zhihu.com/p/591323348

归一化
````````````````````````````````````````````````
BatchNorm

LayerNorm

GroupNorm

+ https://zhuanlan.zhihu.com/p/437446744
+ https://zhuanlan.zhihu.com/p/521535855
+ https://www.itsiwei.com/22671.html
+ https://www.zhihu.com/question/291571486/answer/2746988490
+ https://www.zhihu.com/question/395811291/answer/2398965166
+ https://bbs.huaweicloud.com/blogs/391570#H22?utm_source=oschina&utm_medium=bbs-ex&utm_campaign=other&utm_content=content

激活函数(activation)
````````````````````````````````````````````````

常用激活函数

+ sigmoid
+ ReLU
+ LeakyReLU
+ GeLU
+ SiLU
+ ELU
+ tanh
+ Dropout
+ Softmax
+ Swish
+ Softplus

+ https://zhuanlan.zhihu.com/p/586539385
+ https://www.analyticssteps.com/blogs/7-types-activation-functions-neural-network
+ https://zhuanlan.zhihu.com/p/192497127
+ https://ml-explained.com/blog/activation-functions-explained
+ https://zhuanlan.zhihu.com/p/390990848

全连接层
````````````````````````````````````````````````

又叫Logits

损失函数(loss function)
````````````````````````````````````````````````

+ MSE
+ CrossEntropy
+ BinaryCrossEntropy
+ L1 loss
+ L2 loss
+ triplet loss

+ `19个损失函数汇总，以Pytorch为例 <https://zhuanlan.zhihu.com/p/258395701>`_
+ `Pytorch 中的四种经典 Loss 源码解析 <http://giantpandacv.com/academic/%E7%AE%97%E6%B3%95%E7%A7%91%E6%99%AE/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/Pytorch%E4%B8%AD%E7%9A%84%E5%9B%9B%E7%A7%8D%E7%BB%8F%E5%85%B8Loss%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/>_`
+ `Pytorch常用的交叉熵损失函数CrossEntropyLoss()详解 <https://zhuanlan.zhihu.com/p/98785902>_`

优化器(optimizer)
````````````````````````````````````````````````

+ https://www.cnblogs.com/guoyaohua/p/8542554.html
+ https://zhuanlan.zhihu.com/p/527824835

过拟合与正则化
````````````````````````````````````````````````

https://zhuanlan.zhihu.com/p/259159952

数据增强
````````````````````````````````````````````````

学习率调整
````````````````````````````````````````````````

+ https://zhuanlan.zhihu.com/p/584823280
+ https://zhuanlan.zhihu.com/p/32923584
+ https://zhuanlan.zhihu.com/p/499318751
+ https://zhuanlan.zhihu.com/p/423375831
+ https://zhuanlan.zhihu.com/p/475824165
+ https://zhuanlan.zhihu.com/p/78096138

EMA

+ https://zhuanlan.zhihu.com/p/479898259
+ https://zhuanlan.zhihu.com/p/68748778

分布式训练
````````````````````````````````````````````````

其他
````````````````````````````````````````````````

+ https://zhuanlan.zhihu.com/p/371978706
+ https://www.zhihu.com/question/60751553/answer/1986650670
+ https://www.zhihu.com/question/38002635/answer/2455070318
+ https://zhuanlan.zhihu.com/p/548163311
+ https://zhuanlan.zhihu.com/p/195502362

模型加速技术
------------------------------------------------

+ 梯度累加
+ 混合精度(AMP)
+ 量化
+ 剪枝和蒸馏
+ 稀疏

参考资料
------------------------------------------------

+ https://www.deeplearningbook.org/
+ https://ml-cheatsheet.readthedocs.io/en/latest/